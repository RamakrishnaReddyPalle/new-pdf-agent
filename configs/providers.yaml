# configs/providers.yaml
# Single source of truth for all providers/models.
# Local-first defaults

llm:
  provider: "local_ollama"
  # point to local paths for larger models (downloaded below)
  embedding: "models/embedding/BAAI-bge-large-en-v1.5"
  reranker: "models/reranker/BAAI-bge-reranker-large"

vector:
  provider: "chroma"
  persist_path: "data/artifacts"

ocr:
  provider: "tesseract"
  languages: ["eng"]

pdf2md:
  provider: "marker"

sft:
  llm:
    provider: "ollama"
    ollama:
      url: "http://127.0.0.1:11434"
      model: "llama3.1:8b-instruct-q5_1"
      temperature: 0.2
      max_new_tokens: 512
      connect_timeout: 30     # NEW
      read_timeout: 900       # NEW  (bump to 900 if your first token still stalls)
    company:
      base_url: "https://api.company.ai"
      model: "company-small"
      api_key: "${COMPANY_API_KEY}"

eval_ollama:
  base_url: "http://127.0.0.1:11434"
  model: "llama3.2:latest"
  temperature: 0.2
  max_new_tokens: 256
  connect_timeout: 30
  read_timeout: 600
  retries: 1

embedding:
  # use a larger embedding model (local directory)
  model: "models/embedding/BAAI-bge-large-en-v1.5"
  device: "cpu"    # change to "cuda" if you have GPU

chat:
  sessions_dir: "data/sessions"
  streaming: true
  parallel_per_question: 2
  connect_timeout: 30
  read_timeout: 600

  # Retriever knobs (bigger; aligned to pipeline yaml)
  search_top_k: 36
  rerank_top_k: 24
  return_top_k: 18

  # Splitter knobs
  splitter:
    max_questions: 8
    allow_notes: true
    prompt_path: "configs/prompts/chat/splitter.txt"

  # Guardrail knobs (use intro model for guard; prompt externalized)
  guardrails:
    model_role: "intro"
    prompt_path: "configs/prompts/chat/intro_guard.txt"
    max_input_chars: 6000
    pii_block: true
    pii_regex:
      - "[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}"
      - "\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b"
    blocked_regex:
      - "(?i)hack|exploit|payload"

  # Global structured-output policy
  structured_output:
    prefer_native: true
    method: "json_schema"

  # Prompts (external files, no hardcoding)
  prompts:
    core_rag_path: "configs/prompts/chat/core_rag.txt"
    output_stitch_path: "configs/prompts/chat/output_stitch.txt"

  # Intent policy
  intent_policy:
    table:
      require_tables: false
      prefer: true
      fallback: "text"
    math:
      require_calculators: false
      prefer: true
      fallback: "text"
    glossary:
      fallback: "text"
    meta:
      fallback: "text"

  # Tools
  tools:
    calculators:
      provider: "disabled"
      api:
        base_url: ""
        api_key: ""
        timeout_sec: 20

  # Central switchboard for roles
  models:
    intro:
      provider: "ollama"
      base_url: "http://127.0.0.1:11434"
      model: "llama3.2:latest"
      temperature: 0.2
      max_new_tokens: 256

    splitter:
      provider: "ollama"
      base_url: "http://127.0.0.1:11434"
      model: "llama3.2:latest"
      temperature: 0.2
      max_new_tokens: 256

    # QWEN **only** for CORE
    core:
      provider: "ollama"
      base_url: "http://127.0.0.1:11434"
      model: "qwen2.5:3b"
      temperature: 0.2
      max_new_tokens: 768

    output:
      provider: "ollama"
      base_url: "http://127.0.0.1:11434"
      model: "llama3.2:latest"
      temperature: 0.3
      max_new_tokens: 512

    
    # --- PREVIOUS core ---
    # core:
    #   provider: "hf_local"                # local LoRA (adapter resolved from profile)
    #   base_model_id: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    #   base_model_local_dir: "models/llm/TinyLlama-1.1B-Chat-v1.0"
    #   run_id: "20bb948f"
    #   adapter_from_profile: true
    #   temperature: 0.1
    #   max_new_tokens: 512
    #   device: "cpu"
    # -------------------------------------------------------------------

  # -----------------------------------------------
  # COMPANY SWITCH 
  # -----------------------------------------------

  # models:
  #   intro:
  #     provider: "company"
  #     base_url: "https://api.company.ai"
  #     api_key: "${COMPANY_API_KEY}"
  #     model: "company-small"
  #     temperature: 0.2
  #     max_new_tokens: 256
  #
  #   splitter:
  #     provider: "company"
  #     base_url: "https://api.company.ai"
  #     api_key: "${COMPANY_API_KEY}"
  #     model: "company-small"
  #     temperature: 0.2
  #     max_new_tokens: 256
  #
  #   core:
  #     provider: "company"
  #     base_url: "https://api.company.ai"
  #     api_key: "${COMPANY_API_KEY}"
  #     model: "company-core-8k"
  #     temperature: 0.1
  #     max_new_tokens: 512
  #
  #   output:
  #     provider: "company"
  #     base_url: "https://api.company.ai"
  #     api_key: "${COMPANY_API_KEY}"
  #     model: "company-medium"
  #     temperature: 0.3
  #     max_new_tokens: 512
  # -----------------------------------------------
