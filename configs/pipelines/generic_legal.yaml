# Generic legal documents pipeline configuration.
# configs/pipelines/generic_legal.yaml

ingest:
  ocr:
    enable: true
    dpi: 300
    lang: "eng"
    min_chars_no_ocr: 60
  pdf2md:
    provider: "marker"
    preserve_footnotes: true
    keep_figure_captions: true
  tables:
    structure: true
    provider_order: ["words"]      # robust default; add paddle/tatr later if needed
    paddle:
      enable: false
    words:
      max_cols: 6
      merge_wrap_lines: true
      min_row_height: 6.0
    export_markdown: true
  math:
    enable: true
    min_digits: 2
    require_operator: true
    latex_ocr:
      enable: false
  notations:
    enable: true
    use_llm_adjudication: false

# --- Large, denser chunks (significantly bigger) ---
chunking:
  max_chars: 3200                # BIG chunks
  overlap: 150
  drop_gibberish: true
  drop_toc: true
  min_align_score: 70
  attach_heading_to_body: true   # merge headings into body text
  keep_heading_only: false       # no naked heading chunks

# (optional) hierarchy graph used by summaries/SFT profiling
hierarchy:
  min_node_chars: 400
  use_section_regex: true
  fold_tiny_into_misc: true

# Embedding override (takes precedence over providers.yaml via merge)
embedding:
  model: "models/embedding/BAAI-bge-large-en-v1.5"
  device: "cpu"

index:
  bge_use_prompt: true
  vector:
    max_add_batch: 4000
  # Prevent tiny/heading-only fragments from ever entering the index
  filter:
    min_chars: 240                    # drop ultra-short fragments
    exclude_block_types: ["heading", "image"]

sft:
  generation:
    max_qa: 300                 # go big
    max_summary: 60
    seed: 13
    min_chunk_chars: 300
    max_chunk_chars: 2200

    # Use LLAMA for SFT generation (Qwen is ONLY for core at chat-time)
    use_llm: true
    llm_provider: "ollama"
    llm_model: "llama3.1:8b-instruct-q5_1"
    llm_url: "http://127.0.0.1:11434"
    llm_temperature: 0.2
    llm_max_new_tokens: 320

    # Runner auto-switches to configs/pipelines/prompts/<doc_id>.yaml if present
    prompts_yaml: "configs/pipelines/prompts/default.yaml"
    profile_rules_from_prompts_yaml: true
    profile_rules: {}

    summary_source: "chunks"     # IMPORTANT: ensures summaries get real text
    summary_target_len: 1600
    datasets_root: "data/datasets"

  split:
    train_ratio: 0.85
    dev_ratio: 0.10
    seed: 42

  package:
    format: "alpaca"

# We are NOT fine-tuning now.
finetune:
  backend: "local_peft"
  enable_training: false
  output_root: "data/models"
  datasets_root: "data/datasets"
  register: false

reranker:
  enable: true
  # Bigger CE reranker (local)
  base_model_id: "BAAI/bge-reranker-large"
  base_model_local_dir: "models/reranker/BAAI-bge-reranker-large"
  output_root: "data/reranker"

  mining:
    topk_candidates: 60
    negatives_per_pos: 8
    min_question_len: 8
    max_pairs: 4000
    seed: 123

  train:
    epochs: 1
    batch_size: 16
    lr: 2.0e-5
    warmup_steps: 100
    eval_ratio: 0.1
    seed: 42

eval:
  closed_book:
    use_llm: true
    max_questions: 100
  rag:
    use_llm: true
    max_questions: 100
    top_k: 36           # larger candidate pool to give big chunks room
    rerank_top_k: 24
    return_top_k: 18

chat:
  # Larger search pools; final top-k still reasonable for display
  search_top_k: 36
  rerank_top_k: 24
  return_top_k: 18
