{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd11b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\IIT BBS\\Job Resources\\Business Optima\\new-pdf-agent\n"
     ]
    }
   ],
   "source": [
    "# --- 0) bootstrap ---\n",
    "import os, sys, json, asyncio, time, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while not (PROJECT_ROOT / \"pyproject.toml\").exists() and PROJECT_ROOT != PROJECT_ROOT.parent:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "\n",
    "DOC_ID = \"NFS_2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af0184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> OOS 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT BBS\\Job Resources\\Business Optima\\new-pdf-agent\\packages\\chat\\models.py:164: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPES: ['guard', 'guard_override', 'split', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_final', 'final']\n",
      "— FINAL —\n",
      " I'm ready to assist. What's the question?\n",
      "\n",
      ">>> IN-SCOPE 1\n",
      "TYPES: ['guard', 'split', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_final', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_final', 'final']\n",
      "— FINAL —\n",
      " I'm ready to assist. What is your request?\n",
      "\n",
      ">>> IN-SCOPE 2\n",
      "TYPES: ['guard', 'split', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_token', 'core_final', 'final']\n",
      "— FINAL —\n",
      " I'm ready to assist. What is your request?\n"
     ]
    }
   ],
   "source": [
    "from packages.chat.router import run_turn\n",
    "from packages.chat.memory import SessionStore\n",
    "\n",
    "sess = SessionStore(Path(\"data/sessions\"), DOC_ID).load_or_create()\n",
    "sid = sess.session_id\n",
    "\n",
    "async def run_and_collect(q):\n",
    "    buf, types = \"\", []\n",
    "    async for ev in run_turn(DOC_ID, sid, q):\n",
    "        t = ev.get(\"type\"); types.append(t)\n",
    "        if t == \"final_token\":\n",
    "            tok = ev.get(\"data\",\"\"); buf += tok\n",
    "    print(\"TYPES:\", [t for t in types if t!=\"final_token\"])\n",
    "    print(\"— FINAL —\\n\", buf or \"[no text]\")\n",
    "\n",
    "print(\">>> OOS 1\")\n",
    "await run_and_collect(\"Who won the Premier League in 2021?\")\n",
    "\n",
    "print(\"\\n>>> IN-SCOPE 1\")\n",
    "await run_and_collect(\"Define 'conversion factor' used in this document.\")\n",
    "\n",
    "print(\"\\n>>> IN-SCOPE 2\")\n",
    "await run_and_collect(\"What are Section 3 fees?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b6b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions dir: data\\sessions\n",
      "Adapter used: data\\models\\NFS_2019\\20bb948f\\adapter\n",
      "Collection:   NFS_2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT BBS\\Job Resources\\Business Optima\\new-pdf-agent\\packages\\chat\\models.py:164: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n"
     ]
    }
   ],
   "source": [
    "from packages.chat.router import mount_chat\n",
    "mount = mount_chat(DOC_ID)\n",
    "print(\"Sessions dir:\", mount.sessions_dir)\n",
    "print(\"Adapter used:\", mount.profile.adapter_path)\n",
    "print(\"Collection:  \", mount.profile.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfb3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro     http://127.0.0.1:11434 -> 200\n",
      "splitter  http://127.0.0.1:11434 -> 200\n",
      "core      http://127.0.0.1:11434 -> 200\n",
      "output    http://127.0.0.1:11434 -> 200\n"
     ]
    }
   ],
   "source": [
    "import requests, yaml\n",
    "\n",
    "def check_roles_health(yaml_path=\"configs/providers.yaml\", roles=(\"intro\",\"splitter\",\"core\",\"output\")):\n",
    "    cfg = yaml.safe_load(open(yaml_path, \"r\", encoding=\"utf-8\"))\n",
    "    failed = []\n",
    "    for role in roles:\n",
    "        rcfg = cfg[\"chat\"][\"models\"][role]\n",
    "        if rcfg.get(\"provider\") != \"ollama\":\n",
    "            print(f\"{role:9s} (non-ollama provider) -> skip\")\n",
    "            continue\n",
    "        url = rcfg[\"base_url\"]\n",
    "        try:\n",
    "            r = requests.get(f\"{url}/api/tags\", timeout=10)\n",
    "            print(f\"{role:9s} {url} -> {r.status_code}\")\n",
    "            if r.status_code != 200:\n",
    "                failed.append((role, url, f\"HTTP {r.status_code}\"))\n",
    "        except Exception as e:\n",
    "            print(f\"{role:9s} {url} -> ERROR: {type(e).__name__}: {e}\")\n",
    "            failed.append((role, url, str(e)))\n",
    "    return failed\n",
    "\n",
    "fails = check_roles_health()\n",
    "if fails:\n",
    "    raise SystemExit(\"Ollama health check failed:\\n\" + \"\\n\".join(f\"{r} @ {u}: {e}\" for r,u,e in fails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd2ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session file: data\\sessions\\NFS_2019_83837ce1d49b.json\n"
     ]
    }
   ],
   "source": [
    "from packages.chat.router import run_turn\n",
    "\n",
    "async def run_and_collect(doc_id: str, session_id: str, user_query: str, print_tokens: bool = True):\n",
    "    stream_buf = []\n",
    "    events = []\n",
    "    async for ev in run_turn(doc_id, session_id=session_id, user_query=user_query):\n",
    "        events.append(ev)\n",
    "        t = ev.get(\"type\")\n",
    "        if t == \"final_token\" and print_tokens:\n",
    "            token = ev.get(\"data\", \"\")\n",
    "            stream_buf.append(token)\n",
    "            # lightweight preview\n",
    "            if token.strip():\n",
    "                print(token, end=\"\", flush=True)\n",
    "    if print_tokens:\n",
    "        print(\"\\n\\n— FINAL —\")\n",
    "        print(\"\".join(stream_buf))\n",
    "    return events\n",
    "\n",
    "# fresh session for 7F tests\n",
    "from packages.chat.memory import SessionStore\n",
    "store = SessionStore(mount.sessions_dir, DOC_ID).load_or_create()\n",
    "sid = store.session_id\n",
    "print(\"Session file:\", store.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> OOS\n",
      "This request is out of scope for the selected PDF. (reason: Premier League winner)\n",
      "\n",
      "— FINAL —\n",
      "This request is out of scope for the selected PDF. (reason: Premier League winner)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['guard', 'final']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\">>> OOS\")\n",
    "evs_oos = await run_and_collect(DOC_ID, sid, \"Who won the Premier League in 2021?\")\n",
    "# Inspect non-token events you might want to log/trace\n",
    "[e[\"type\"] for e in evs_oos if e[\"type\"] != \"final_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276d7106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> In-scope\n",
      "This request is out of scope for the selected PDF.\n",
      "\n",
      "— FINAL —\n",
      "This request is out of scope for the selected PDF.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['guard',\n",
       " 'split',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_token',\n",
       " 'core_final',\n",
       " 'final']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\">>> In-scope\")\n",
    "evs_in = await run_and_collect(DOC_ID, sid, \"Define 'conversion factor' used in this document.\")\n",
    "# Quick peek at event types\n",
    "[e[\"type\"] for e in evs_in if e[\"type\"] != \"final_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e378da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " There was no prior summary. The user asked about the winner of the Premier League in 2021, and I replied that I couldn't provide an accurate answer as it's out of scope. I suggested checking the official Premier League website or sports news websites for more information.\n",
      "\n",
      "Tail turns: ['[user] Who won the Premier League in 2021?', \"[assistant] Unfortunately, I don't have enough information to provide an accurate response. \", \"[user] Define 'conversion factor' used in this document.\", \"[assistant] Unfortunately, I don't have enough information to provide the winner of the Prem\"]\n"
     ]
    }
   ],
   "source": [
    "from packages.chat.memory import SessionStore\n",
    "store2 = SessionStore(mount.sessions_dir, DOC_ID, session_id=sid).load_or_create()\n",
    "print(\"Summary:\\n\", store2.state.summary)\n",
    "print(\"\\nTail turns:\", [f\"[{t.role}] {t.content[:80]}\" for t in store2.last_n(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af31843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/tmp/last_final.txt\n"
     ]
    }
   ],
   "source": [
    "# Collect final text from prior run (evs_in) if needed:\n",
    "final_text = \"\".join(ev[\"data\"] for ev in evs_in if ev[\"type\"] == \"final_token\").strip()\n",
    "Path(\"data/tmp\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/tmp/last_final.txt\").write_text(final_text, encoding=\"utf-8\")\n",
    "print(\"Saved:\", \"data/tmp/last_final.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1372a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-agent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
