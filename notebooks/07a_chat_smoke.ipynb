{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5209d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: d:\\IIT BBS\\Job Resources\\Business Optima\\new-pdf-agent\n"
     ]
    }
   ],
   "source": [
    "# --- 0) bootstrap ---\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "while not (ROOT / \"pyproject.toml\").exists() and ROOT != ROOT.parent:\n",
    "    ROOT = ROOT.parent\n",
    "os.chdir(ROOT)\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "print(\"Project root:\", ROOT)\n",
    "\n",
    "DOC_ID = \"NFS_2019\"      # <- change doc here to mount any ingested PDF\n",
    "ARTIFACTS_ROOT = Path(\"data/artifacts\")\n",
    "MODELS_ROOT    = Path(\"data/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3b0aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML run_id: 20bb948f\n",
      "Candidate adapter: data\\models\\NFS_2019\\20bb948f\\adapter\n"
     ]
    }
   ],
   "source": [
    "# --- 1) resolve run_id from YAML (optional pin) and build adapter path hint ---\n",
    "from packages.core_config.config import load_yaml\n",
    "\n",
    "cfg = load_yaml(\"configs/providers.yaml\")\n",
    "yaml_run_id = cfg.get(\"chat.models.core.run_id\")\n",
    "print(\"YAML run_id:\", yaml_run_id)\n",
    "\n",
    "def candidate_adapter(doc_id: str, run_id: str) -> Path:\n",
    "    # prefer adapter/ over hf_out/\n",
    "    a = MODELS_ROOT / doc_id / run_id / \"adapter\"\n",
    "    if a.exists():\n",
    "        return a\n",
    "    return MODELS_ROOT / doc_id / run_id / \"hf_out\"\n",
    "\n",
    "cand = candidate_adapter(DOC_ID, yaml_run_id) if yaml_run_id else None\n",
    "print(\"Candidate adapter:\", cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def63fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro     http://127.0.0.1:11434 -> 200\n",
      "splitter  http://127.0.0.1:11434 -> 200\n",
      "output    http://127.0.0.1:11434 -> 200\n"
     ]
    }
   ],
   "source": [
    "# --- 2) Ollama health check for the chat roles (intro/splitter/output) ---\n",
    "import requests, yaml\n",
    "\n",
    "def check_roles_health(yaml_path=\"configs/providers.yaml\", roles=(\"intro\",\"splitter\",\"output\")):\n",
    "    cfg = yaml.safe_load(open(yaml_path, \"r\", encoding=\"utf-8\"))\n",
    "    failed = []\n",
    "    for role in roles:\n",
    "        url = cfg[\"chat\"][\"models\"][role][\"base_url\"]\n",
    "        try:\n",
    "            r = requests.get(f\"{url}/api/tags\", timeout=10)\n",
    "            print(f\"{role:9s} {url} -> {r.status_code}\")\n",
    "            if r.status_code != 200:\n",
    "                failed.append((role, url, f\"HTTP {r.status_code}\"))\n",
    "        except Exception as e:\n",
    "            print(f\"{role:9s} {url} -> ERROR: {type(e).__name__}: {e}\")\n",
    "            failed.append((role, url, str(e)))\n",
    "    return failed\n",
    "\n",
    "fails = check_roles_health()\n",
    "if fails:\n",
    "    raise SystemExit(\n",
    "        \"Ollama health check failed for roles:\\n\" +\n",
    "        \"\\n\".join(f\" - {role} at {url}: {err}\" for role, url, err in fails) +\n",
    "        \"\\n\\nQuick fixes:\\n\"\n",
    "        \"  1) Ensure 'ollama serve' is running on that port\\n\"\n",
    "        \"  2) Make YAML base_url match the running port (e.g., http://127.0.0.1:11434)\\n\"\n",
    "        \"  3) In *this* terminal, you can also set:\\n\"\n",
    "        \"       set OLLAMA_HOST=http://127.0.0.1:11434   (cmd)\\n\"\n",
    "        \"       $env:OLLAMA_HOST = 'http://127.0.0.1:11434'   (PowerShell)\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0655aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected collection: NFS_2019\n",
      "Selected adapter   : data\\models\\NFS_2019\\20bb948f\\adapter\n",
      "Selected run_id    : 20bb948f\n"
     ]
    }
   ],
   "source": [
    "# --- 3) inspect what loader will pick (even without profile.json) ---\n",
    "from packages.chat.router import load_profile\n",
    "\n",
    "probe = load_profile(DOC_ID, artifacts_root=ARTIFACTS_ROOT)  # auto-discovers under data/models/<doc_id>\n",
    "print(\"Selected collection:\", probe.collection)\n",
    "print(\"Selected adapter   :\", probe.adapter_path)\n",
    "print(\"Selected run_id    :\", getattr(probe, \"adapter_run_id\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1106b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT BBS\\Job Resources\\Business Optima\\new-pdf-agent\\packages\\chat\\models.py:164: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  return ChatOllama(\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions dir: data\\sessions\n",
      "Adapter used: data\\models\\NFS_2019\\20bb948f\\adapter\n",
      "Collection:   NFS_2019\n",
      "Tools:        ['doc_retrieve', 'calc_run']\n"
     ]
    }
   ],
   "source": [
    "# --- 4) mount chat (models + retriever + tools) ---\n",
    "from packages.chat.router import mount_chat\n",
    "\n",
    "mount = mount_chat(DOC_ID)  # uses providers.yaml & profile/auto-discovery\n",
    "print(\"Sessions dir:\", mount.sessions_dir)\n",
    "print(\"Adapter used:\", mount.profile.adapter_path)\n",
    "print(\"Collection:  \", mount.profile.collection)\n",
    "print(\"Tools:       \", [t.name for t in mount.tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da61c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 hits\n",
      " 1. id=NFS_2019-11267 score=0.650 page=None, heading=FEE SCHEDULE > N 25 8 28 3\n",
      " 2. id=NFS_2019-h-207 score=0.642 page=None, heading=FEE SCHEDULE > 3.76 BR XXX XXX\n",
      " 3. id=NFS_2019-11118 score=0.642 page=None, heading=FEE SCHEDULE > CONVERSION FACTORS\n",
      " 4. id=NFS_2019-11334 score=0.640 page=None, heading=FEE SCHEDULE > N 25 8 28 3\n",
      " 5. id=NFS_2019-11404 score=0.640 page=None, heading=FEE SCHEDULE > N 25 8 28 3\n",
      " 6. id=NFS_2019-488 score=0.640 page=None, heading=FEE SCHEDULE > GENERAL GROUND RULES\n"
     ]
    }
   ],
   "source": [
    "# --- 5) retriever smoke test ---\n",
    "hits = mount.retriever.search(\"What are the Section 3 fees?\")\n",
    "print(\"Top\", len(hits), \"hits\")\n",
    "for i, h in enumerate(hits, 1):\n",
    "    m = h.get(\"metadata\", {})\n",
    "    print(f\"{i:>2}. id={h['id']} score={h['score']:.3f} page={m.get('page')}, heading={m.get('heading_path')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbff6db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['snippets', 'citations'])\n",
      "Citations sample: [{'id': 'NFS_2019-11284', 'page': None, 'heading_path': 'FEE SCHEDULE > N 25 8 28 3', 'table_id': None, 'score': 0.7133106227939584}, {'id': 'NFS_2019-11334', 'page': None, 'heading_path': 'FEE SCHEDULE > N 25 8 28 3', 'table_id': None, 'score': 0.7132730034384358}]\n"
     ]
    }
   ],
   "source": [
    "# --- 6) retriever 'tool' (LangChain StructuredTool) smoke test ---\n",
    "tool = mount.retriever_tool\n",
    "tool_out = tool.invoke({\"query\": \"Provide summary of Section 3 fee schedule\", \"top_k\": 3})\n",
    "print(tool_out.keys())\n",
    "print(\"Citations sample:\", tool_out[\"citations\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715ca767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session file: data\\sessions\\NFS_2019_7f75880dff8b.json\n"
     ]
    }
   ],
   "source": [
    "# --- 7) session store + summary buffer (phase 7.0 memory) ---\n",
    "from packages.chat.memory import SessionStore, SummaryBuffer\n",
    "\n",
    "store = SessionStore(mount.sessions_dir, doc_id=mount.profile.doc_id).load_or_create()\n",
    "buf   = SummaryBuffer(store, llm=mount.llm_output)\n",
    "\n",
    "store.append(\"user\", \"Hi, can you help me understand section 3 fees?\")\n",
    "store.append(\"assistant\", \"Sureâ€”what specifically about section 3?\")\n",
    "buf.maybe_summarize()\n",
    "\n",
    "print(\"Session file:\", store.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4afb31a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intro   : Ready.\n",
      "splitter: I'm happy to help, but I don't see a question. Could you please provide more context or clarify what you're asking about \"A\" and \"B\"? Are th\n",
      "core    : Say 'ready core'.\n",
      "\n",
      "2. The \"ready core\" is a technique used by professional wrestlers to perform a series of moves in a short amount of time.\n",
      "output  : It seems like you're asking me to summarize a sequence of states, but I'm not sure what the context is. Could you provide more information o\n"
     ]
    }
   ],
   "source": [
    "# --- 8) Optional: quick sanity pings (won't crash if Ollama is down) ---\n",
    "def safe_ping(llm, prompt):\n",
    "    try:\n",
    "        return llm.invoke(prompt).content[:140]\n",
    "    except Exception as e:\n",
    "        return f\"[unavailable: {type(e).__name__}]\"\n",
    "\n",
    "print(\"intro   :\", safe_ping(mount.llm_intro, \"Say 'ready'.\"))\n",
    "print(\"splitter:\", safe_ping(mount.llm_splitter, \"Split: A and B?\"))\n",
    "print(\"core    :\", safe_ping(mount.llm_core, \"Say 'ready core'.\"))\n",
    "print(\"output  :\", safe_ping(mount.llm_output, \"Summarize: ready -> ok\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-agent-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
